<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction To Linear Algebra | Applied Statistical Methods in Animal Sciences</title>
  <meta name="description" content="Chapter 2 Introduction To Linear Algebra | Applied Statistical Methods in Animal Sciences" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction To Linear Algebra | Applied Statistical Methods in Animal Sciences" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/charlotte-ngs/gelasmss2021" />
  
  
  <meta name="github-repo" content="charlotte-ngs/gelasmss2021" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction To Linear Algebra | Applied Statistical Methods in Animal Sciences" />
  
  
  

<meta name="author" content="Peter von Rohr" />


<meta name="date" content="2021-02-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="asm-intro.html"/>
<link rel="next" href="quan-gen.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Statistical Methods</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#general-developments"><i class="fa fa-check"></i>General Developments</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#where-does-this-course-fit-in"><i class="fa fa-check"></i>Where Does This Course Fit In?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-objectives"><i class="fa fa-check"></i>Course Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="asm-intro.html"><a href="asm-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="asm-intro.html"><a href="asm-intro.html#us-presidential-campaigns"><i class="fa fa-check"></i><b>1.1</b> US Presidential Campaigns</a></li>
<li class="chapter" data-level="1.2" data-path="asm-intro.html"><a href="asm-intro.html#health-care"><i class="fa fa-check"></i><b>1.2</b> Health Care</a></li>
<li class="chapter" data-level="1.3" data-path="asm-intro.html"><a href="asm-intro.html#face-recognition"><i class="fa fa-check"></i><b>1.3</b> Face Recognition</a></li>
<li class="chapter" data-level="1.4" data-path="asm-intro.html"><a href="asm-intro.html#feed-intake-and-behavior-traits-of-cows"><i class="fa fa-check"></i><b>1.4</b> Feed Intake and Behavior Traits of Cows</a></li>
<li class="chapter" data-level="1.5" data-path="asm-intro.html"><a href="asm-intro.html#conclusions-from-examples"><i class="fa fa-check"></i><b>1.5</b> Conclusions from Examples</a></li>
<li class="chapter" data-level="1.6" data-path="asm-intro.html"><a href="asm-intro.html#asm-traditional-animal-breeding"><i class="fa fa-check"></i><b>1.6</b> Traditional Livestock Breeding</a></li>
<li class="chapter" data-level="1.7" data-path="asm-intro.html"><a href="asm-intro.html#asm-genomic-selection"><i class="fa fa-check"></i><b>1.7</b> Genomic Selection</a></li>
<li class="chapter" data-level="1.8" data-path="asm-intro.html"><a href="asm-intro.html#asm-mono-genic-model"><i class="fa fa-check"></i><b>1.8</b> Mono-Genic Model</a></li>
<li class="chapter" data-level="1.9" data-path="asm-intro.html"><a href="asm-intro.html#asm-two-step-approach"><i class="fa fa-check"></i><b>1.9</b> Two Step Approach</a></li>
<li class="chapter" data-level="1.10" data-path="asm-intro.html"><a href="asm-intro.html#asm-single-step-approach"><i class="fa fa-check"></i><b>1.10</b> Single Step Approach</a></li>
<li class="chapter" data-level="1.11" data-path="asm-intro.html"><a href="asm-intro.html#asm-summary"><i class="fa fa-check"></i><b>1.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro-linalg.html"><a href="intro-linalg.html"><i class="fa fa-check"></i><b>2</b> Introduction To Linear Algebra</a><ul>
<li class="chapter" data-level="2.1" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-glimpse-ahead"><i class="fa fa-check"></i><b>2.1</b> Glimpse Ahead</a></li>
<li class="chapter" data-level="2.2" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-vectors"><i class="fa fa-check"></i><b>2.2</b> Vectors</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-physics-perspective"><i class="fa fa-check"></i><b>2.2.1</b> Physics Perspective</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-computer-science-perspective"><i class="fa fa-check"></i><b>2.2.2</b> Computer Science Perspective</a></li>
<li class="chapter" data-level="2.2.3" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-geometric-context"><i class="fa fa-check"></i><b>2.2.3</b> Geometric Context</a></li>
<li class="chapter" data-level="2.2.4" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-coordinate-system"><i class="fa fa-check"></i><b>2.2.4</b> Coordinate System</a></li>
<li class="chapter" data-level="2.2.5" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-vector-operations"><i class="fa fa-check"></i><b>2.2.5</b> Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-matrices"><i class="fa fa-check"></i><b>2.3</b> Matrices</a><ul>
<li class="chapter" data-level="2.3.1" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-special-matrices"><i class="fa fa-check"></i><b>2.3.1</b> Special Matrices</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-matrix-operation"><i class="fa fa-check"></i><b>2.3.2</b> Matrix Operations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-systems-of-equations"><i class="fa fa-check"></i><b>2.4</b> Systems Of Equations</a><ul>
<li class="chapter" data-level="2.4.1" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-matrix-vector-notation"><i class="fa fa-check"></i><b>2.4.1</b> Matrix-Vector Notation</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="intro-linalg.html"><a href="intro-linalg.html#intro-linalg-solving-systems-of-linear-equations"><i class="fa fa-check"></i><b>2.5</b> Solving Systems of Linear Equations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="quan-gen.html"><a href="quan-gen.html"><i class="fa fa-check"></i><b>3</b> Basics in Quantitative Genetics</a><ul>
<li class="chapter" data-level="3.1" data-path="quan-gen.html"><a href="quan-gen.html#single-locus-quant-trait"><i class="fa fa-check"></i><b>3.1</b> Single Locus - Quantitative Trait</a><ul>
<li class="chapter" data-level="3.1.1" data-path="quan-gen.html"><a href="quan-gen.html#qg-terminology"><i class="fa fa-check"></i><b>3.1.1</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="quan-gen.html"><a href="quan-gen.html#qg-frequency"><i class="fa fa-check"></i><b>3.2</b> Frequencies</a></li>
<li class="chapter" data-level="3.3" data-path="quan-gen.html"><a href="quan-gen.html#hw-eq"><i class="fa fa-check"></i><b>3.3</b> Hardy-Weinberg Equilibrium</a></li>
<li class="chapter" data-level="3.4" data-path="quan-gen.html"><a href="quan-gen.html#value-mean"><i class="fa fa-check"></i><b>3.4</b> Value and Mean</a><ul>
<li class="chapter" data-level="3.4.1" data-path="quan-gen.html"><a href="quan-gen.html#geno-value"><i class="fa fa-check"></i><b>3.4.1</b> Genotypic Values</a></li>
<li class="chapter" data-level="3.4.2" data-path="quan-gen.html"><a href="quan-gen.html#pop-mean"><i class="fa fa-check"></i><b>3.4.2</b> Population Mean</a></li>
<li class="chapter" data-level="3.4.3" data-path="quan-gen.html"><a href="quan-gen.html#breed-value"><i class="fa fa-check"></i><b>3.4.3</b> Breeding Values</a></li>
<li class="chapter" data-level="3.4.4" data-path="quan-gen.html"><a href="quan-gen.html#allele-substitution"><i class="fa fa-check"></i><b>3.4.4</b> Allele Substitution</a></li>
<li class="chapter" data-level="3.4.5" data-path="quan-gen.html"><a href="quan-gen.html#dominance-deviation"><i class="fa fa-check"></i><b>3.4.5</b> Dominance Deviation</a></li>
<li class="chapter" data-level="3.4.6" data-path="quan-gen.html"><a href="quan-gen.html#summary-of-values"><i class="fa fa-check"></i><b>3.4.6</b> Summary of Values</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="quan-gen.html"><a href="quan-gen.html#variances"><i class="fa fa-check"></i><b>3.5</b> Variances</a></li>
<li class="chapter" data-level="3.6" data-path="quan-gen.html"><a href="quan-gen.html#extension-to-more-loci"><i class="fa fa-check"></i><b>3.6</b> Extension To More Loci</a><ul>
<li class="chapter" data-level="3.6.1" data-path="quan-gen.html"><a href="quan-gen.html#epistatic-interaction"><i class="fa fa-check"></i><b>3.6.1</b> Epistatic Interaction</a></li>
<li class="chapter" data-level="3.6.2" data-path="quan-gen.html"><a href="quan-gen.html#interaction-variance"><i class="fa fa-check"></i><b>3.6.2</b> Interaction Variance</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="quan-gen.html"><a href="quan-gen.html#genetic-models"><i class="fa fa-check"></i><b>3.7</b> Genetic Models</a><ul>
<li class="chapter" data-level="3.7.1" data-path="quan-gen.html"><a href="quan-gen.html#model-usage-in-routine-evaluations"><i class="fa fa-check"></i><b>3.7.1</b> Model Usage In Routine Evaluations</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="quan-gen.html"><a href="quan-gen.html#appendix-derivations"><i class="fa fa-check"></i><b>3.8</b> Appendix: Derivations</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Statistical Methods in Animal Sciences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro-linalg" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Introduction To Linear Algebra</h1>
<p>Linear Algebra is a large area from which we only need the following three topics</p>
<ol style="list-style-type: decimal">
<li>Vectors</li>
<li>Matrices and</li>
<li>Systems of linear equations.</li>
</ol>
<div id="intro-linalg-glimpse-ahead" class="section level2">
<h2><span class="header-section-number">2.1</span> Glimpse Ahead</h2>
<p>The central topic of this course is the prediction of breeding values. Most approaches to predict breeding values require the solution of large systems of linear equations. These systems of equations are written down using vectors and matrices. Hence the three mentioned topics are important to understand at a level that they can be used as tools for the prediction of breeding values.</p>
</div>
<div id="intro-linalg-vectors" class="section level2">
<h2><span class="header-section-number">2.2</span> Vectors</h2>
<p>The material of this section is largely based on the video tutorial (<a href="https://youtu.be/fNk_zzaMoSs" class="uri">https://youtu.be/fNk_zzaMoSs</a>) from <span class="citation">(3blue1brown <a href="#ref-3blue1brown2016" role="doc-biblioref">2016</a>)</span>. We try to give a summarized transcript of the video. The vector is the fundamental building block of linear algebra. There are three different but related concepts about what vectors are. We call them</p>
<ol style="list-style-type: decimal">
<li>the physics perspective</li>
<li>the computer science perspective and</li>
<li>the mathematics perspective.</li>
</ol>
<p>The mathematics perspective tries to provide a very general concept, saying that anything can be a vector as long as, one can add two vectors or a vector can be multiplied by a factor and the result of both operations is a vector again. For what we want to use vectors for in the context of livestock breeding and genomics, the mathematics perspective is not so useful, hence we ignore it from now on.</p>
<div id="intro-linalg-physics-perspective" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Physics Perspective</h3>
<p>The physics perspective is that vectors are arrows with a certain <strong>length</strong> and a <strong>direction</strong> they are pointing to. As long as length and direction are the same, the arrows can be moved around and they are still the same vector. Different arrows with the same length and the same direction are called <strong>representatives</strong> of the same vector. Vectors that are in a flat plane are called two-dimensional. Those who are sitting in the same Euclidean space that we are all living in, are called three-dimensional.</p>
<p><img src="odg/vector-physics-perspective.png" width="381" /></p>
</div>
<div id="intro-linalg-computer-science-perspective" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Computer Science Perspective</h3>
<p>In the computer science perspective vectors are ordered list of numbers. Later we will see that vectors can also contain more general objects like strings. As an example, we assume that we are analyzing carcasses and the only thing we know about a carcass is its slaughter-weight (SW) and its price (P). The different carcasses can then be represented by a pair of numbers the first being the slaughter-weight and the second being the price. It is important to note here, that the order of the number matters. In terms of vectors, here each carcass is represented by a two-dimensional vector.</p>
<p><img src="odg/vector-cs-perspecitve.png" width="440" /></p>
</div>
<div id="intro-linalg-geometric-context" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Geometric Context</h3>
<p>Some basic properties of vectors are introduced using the geometric context, that a vector is an arrow located in a certain coordinate system with its tail sitting at the origin of the coordinate system. This is a little bit different from the physics perspective (see <a href="intro-linalg.html#intro-linalg-physics-perspective">2.2.1</a>) where the arrow can sit anywhere in space. In linear algebra it is almost always the case that vectors are rooted at the origin. Once we understand the properties of vectors in the context of arrows in space, we can then translate these properties to the list-of-numbers point of view (see <a href="intro-linalg.html#intro-linalg-computer-science-perspective">2.2.2</a>) considering the coordinates of the vectors.</p>
</div>
<div id="intro-linalg-coordinate-system" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Coordinate System</h3>
<p>It is important to introduce the coordinate system, because this will be the basis of the correspondence between the two perspectives of linear algebra. For the moment, we focus on two dimensions. The horizontal line is called the x-axis and the vertical line is called the y-axis. The place where the two lines intersect is called the origin. An arbitrary length is chosen to represent <span class="math inline">\(1\)</span>. The coordinates of a vector is a pair of numbers that give instructions for how to get from the tail of that vector at the origin to its tip. The first number tells you how far to walk along the x-axis (positive numbers indicating rightward motion, negative numbers indicating leftward motion) and the second number tell you how far to walk parallel to the y-axis (positive numbers indicating upward motion, negative numbers indicating downward motion).</p>
<p><img src="odg/coordinate-system.png" width="440" /></p>
</div>
<div id="intro-linalg-vector-operations" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Vector Operations</h3>
<p>The vectors by themselves can be pretty interesting objects, but they get really useful when considering some operations that we can perform on them. Here we consider three basic operations.</p>
<ol style="list-style-type: decimal">
<li>addition</li>
<li>multiplication by a scalar number and</li>
<li>dot product</li>
</ol>
<div id="intro-linalg-vector-addition" class="section level4">
<h4><span class="header-section-number">2.2.5.1</span> Addition</h4>
<p>Let us assume, we have two vectors <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span>. To add these two vectors, move the second one such that its tail sits at the tip of the first one. Then draw a new vector from the tail of the first one to the tip of the second one. The new vector corresponds to the sum of the two vectors (Figure <a href="intro-linalg.html#fig:vector-sum">2.1</a>).</p>
<div class="figure"><span id="fig:vector-sum"></span>
<img src="odg/vector-sum.png" alt="Addition of two vectors" width="440" />
<p class="caption">
Figure 2.1: Addition of two vectors
</p>
</div>
<p>Numerically, vector addition corresponds to summing up each of the coordinates individually. Hence if we have two vectors <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> with their coordinates given as</p>
<p><span class="math display">\[v = \left[\begin{array}{c} v_x \\ v_y \end{array}\right] \text{, } w = \left[\begin{array}{c} w_x \\ w_y \end{array}\right]\]</span></p>
<p>then the sum <span class="math inline">\(v+w\)</span> has coordinates</p>
<p><span class="math display">\[v+w =  \left[\begin{array}{c} v_x + w_x \\ v_y+w_y \end{array}\right]\]</span></p>
</div>
<div id="intro-linalg-vector-scalar-multiplication" class="section level4">
<h4><span class="header-section-number">2.2.5.2</span> Multiplication by a Scalar Number</h4>
<p>This operation is best understood by looking at a few examples. If we take the number <span class="math inline">\(2\)</span> and multiply it by a certain vector <span class="math inline">\(v\)</span>, this means that we stretch out the vector <span class="math inline">\(v\)</span> such that it is <span class="math inline">\(2\)</span> times as long as the original vector. Multiplication of a vector with positive numbers does not change the direction of the vector. Multiplying a vector <span class="math inline">\(v\)</span> with a negative number like <span class="math inline">\(-0.5\)</span> then the direction gets flipped around and then squished by <span class="math inline">\(0.5\)</span>.</p>
<p><img src="odg/vector-scalar-multiplication.png" width="381" /></p>
<p>The operation of multiplying a vector by a given number, like <span class="math inline">\(2\)</span> or <span class="math inline">\(-0.5\)</span> is also called <strong>scaling</strong> and that is the reason why in linear algebra the numbers like <span class="math inline">\(2\)</span> and <span class="math inline">\(-0.5\)</span> are called <strong>scalar</strong> numbers or just scalars. Numerically, stretching a vector by a given number like <span class="math inline">\(2\)</span>, corresponds to multiplying each of the coordinate components by that factor <span class="math inline">\(2\)</span>. For a vector <span class="math inline">\(v\)</span> with coordinate components <span class="math inline">\(v_x\)</span> and <span class="math inline">\(v_y\)</span>, the vector <span class="math inline">\(2v\)</span> has coordinates <span class="math inline">\(2v_x\)</span> and <span class="math inline">\(2v_y\)</span></p>
<p><span class="math display">\[v = \left[\begin{array}{c} v_x \\ v_y \end{array}\right] \text{, }\quad 2v = \left[\begin{array}{c} 2v_x \\ 2v_y \end{array}\right]\]</span></p>
</div>
<div id="intro-linalg-dot-product" class="section level4">
<h4><span class="header-section-number">2.2.5.3</span> Dot Product</h4>
<p>The dot product is explained in a different video that can be seen on <a href="https://youtu.be/LyGKycYT2v0" class="uri">https://youtu.be/LyGKycYT2v0</a>. Numerically, if you have two vectors of the same dimension, meaning two lists of numbers of the same length, e.g. <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> then their dot product <span class="math inline">\(v \cdot w\)</span> can be computed by pairing up all of the coordinates, multiplying these pairs together and adding the result. So the vectors</p>
<p><span class="math display">\[v = \left[\begin{array}{c} v_x \\ v_y \end{array}\right] \text{ and } w= \left[\begin{array}{c} w_x \\ w_y \end{array}\right]\]</span></p>
<p>their dot product <span class="math inline">\(v \cdot w\)</span> then is computed as</p>
<p><span class="math display">\[v \cdot w = v_x * w_x + v_y * w_y\]</span></p>
</div>
</div>
</div>
<div id="intro-linalg-matrices" class="section level2">
<h2><span class="header-section-number">2.3</span> Matrices</h2>
<p>The introduction to the topic of matrices is available from <a href="https://youtu.be/kYB8IZa5AuE" class="uri">https://youtu.be/kYB8IZa5AuE</a> and <a href="https://youtu.be/XkY2DOUCWMU" class="uri">https://youtu.be/XkY2DOUCWMU</a>. An <span class="math inline">\(m \times n\)</span> matrix is a table-like object of <span class="math inline">\(m*n\)</span> numbers arranged in <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns. In general the <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> has the following structure.</p>
<p><span class="math display">\[
A = \left[
\begin{array}{cccc}
a_{11}  &amp;  a_{12} &amp;  \ldots  &amp;  a_{1n} \\
a_{21}  &amp;  a_{22} &amp;  \ldots  &amp;  a_{2n} \\
\vdots  &amp;         &amp;          &amp;  \vdots \\
a_{m1}  &amp;  a_{m2} &amp;  \ldots  &amp;  a_{mn}
\end{array}
\right]
\]</span></p>
<p>The <span class="math inline">\(m*n\)</span> numbers inside of the square brackets are called elements of the matrix. The element of matrix <span class="math inline">\(A\)</span> that is in row <span class="math inline">\(i\)</span> and in column <span class="math inline">\(j\)</span> is called <span class="math inline">\(a_{ij}\)</span> or <span class="math inline">\((A)_{ij}\)</span>. As an example</p>
<p><span class="math display">\[
A =  \left[
\begin{array}{ccc}
2  &amp;  3  &amp;  1  \\
5  &amp;  1  &amp;  2
\end{array}
\right]
\]</span>
is a <span class="math inline">\(2 \times 3\)</span> matrix. In the first row the second element corresponds to <span class="math inline">\((A)_{12} = a_{12} = 3\)</span>. An <span class="math inline">\(n\times n\)</span> matrix (i.e. a matrix with equal numbers of rows and columns) is called a <strong>quadratic</strong> matrix. Two matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are called <strong>equal</strong>, if they have the same number of rows and columns and if the corresponding elements are the same, i.e.</p>
<p><span class="math display">\[
(A)_{ij} = (B)_{ij} \text{ for all i and j}
\]</span></p>
<div id="intro-linalg-special-matrices" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Special Matrices</h3>
<p>The following matrices are special and are used in special cases.</p>
<ul>
<li><strong>Nullmatrix</strong>: The <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(0\)</span> is called Nullmatrix, if each element is equal to zero.</li>
<li><strong>Upper Triangular Matrix</strong>: The square matrix <span class="math inline">\(R\)</span> is called upper triangular matrix, if <span class="math inline">\((R)_{ij} = 0\)</span> for <span class="math inline">\(i&gt;j\)</span>.</li>
<li><strong>Lower Triangular Matrix</strong>: The square matrix <span class="math inline">\(L\)</span> is called lower triangular matrix, if <span class="math inline">\((L)_{ij} = 0\)</span> for <span class="math inline">\(i&lt;j\)</span>.</li>
<li><strong>Diagonal Matrix</strong>: The square matrix <span class="math inline">\(D\)</span> is called diagonal matrix, if <span class="math inline">\((D)_{ij} = 0\)</span> for <span class="math inline">\(i\ne j\)</span>.</li>
<li><strong>Identity Matrix</strong>: The diagonal matrix <span class="math inline">\(I\)</span> is called identity matrix, if all diagonal elements <span class="math inline">\((I)_{ii} = 1\)</span>.</li>
<li><strong>Column Vector</strong>: A <span class="math inline">\(m\times 1\)</span> matrix is often called a column vector.</li>
<li><strong>Row Vector</strong>: A <span class="math inline">\(1\times n\)</span> matrix is is often called a row vector.</li>
</ul>
</div>
<div id="intro-linalg-matrix-operation" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Matrix Operations</h3>
<p>The following operations with matrices are defined.</p>
<div id="intro-linalg-matrix-addition" class="section level4">
<h4><span class="header-section-number">2.3.2.1</span> Addition</h4>
<p>For two <span class="math inline">\(m\times n\)</span> matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, their sum <span class="math inline">\(A+B\)</span> is again a <span class="math inline">\(m\times n\)</span> matrix with each element corresponding to the sum of the corresponding elements from <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Hence, we can write</p>
<p><span class="math display">\[(A+B)_{ij} = (A)_{ij} + (B)_{ij} \text{ for all i and j}\]</span></p>
</div>
<div id="intro-linalg-matrix-multiplication-with-number" class="section level4">
<h4><span class="header-section-number">2.3.2.2</span> Multiplication with a Number</h4>
<p>A <span class="math inline">\(m\times n\)</span> matrix A is multiplied by a number <span class="math inline">\(\alpha\)</span> by multiplying every element <span class="math inline">\((A)_{ij}\)</span> of <span class="math inline">\(A\)</span> with <span class="math inline">\(\alpha\)</span>. The result <span class="math inline">\(\alpha * A\)</span> is computed as <span class="math inline">\((\alpha * A)_{ij} = \alpha * (A)_{ij}\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p>
</div>
<div id="intro-linalg-multiplication-two-matrices" class="section level4">
<h4><span class="header-section-number">2.3.2.3</span> Multiplication of two Matrices</h4>
<p>Given a <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(A\)</span> and a <span class="math inline">\(n\times p\)</span> matrix <span class="math inline">\(B\)</span>, their matrix product <span class="math inline">\(AB\)</span> is a <span class="math inline">\(m\times p\)</span> matrix with</p>
<p><span class="math display">\[ (AB)_{ij} = \sum_{k=1}^n (A)_{ik} * (B)_{kj} = (A)_{i1} * (B)_{1j} + (A)_{i2} * (B)_{2j} + \ldots + (A)_{in} * (B)_{nj}\]</span></p>
</div>
<div id="intro-linalg-laws-matrix-operations" class="section level4">
<h4><span class="header-section-number">2.3.2.4</span> Laws of Matrix Operations</h4>
<ul>
<li><strong>Commutativity</strong>: For two <span class="math inline">\(m\times n\)</span> matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> the addition is commutative, i.e. <span class="math inline">\(A + B = B + A\)</span>.</li>
<li><strong>Associativity of addition</strong>: For <span class="math inline">\(m\times n\)</span> matrices <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span>, the addition is associative, i.e., <span class="math inline">\(A + (B + C) = (A + B) + C\)</span></li>
<li><strong>Associativity of multiplication</strong>: For a <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(A\)</span>, a <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(B\)</span> and a <span class="math inline">\(p \times q\)</span> matrix <span class="math inline">\(C\)</span>, the multiplication is associative, i.e., <span class="math inline">\(A(BC) = (AB)C\)</span></li>
<li><strong>Distributivity</strong>: For <span class="math inline">\(m\times n\)</span> matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> and <span class="math inline">\(n\times p\)</span> matrices <span class="math inline">\(C\)</span> and <span class="math inline">\(D\)</span>, the distributive law holds, i.e., <span class="math inline">\((A+B)C = AC + BC\)</span> and <span class="math inline">\(A(C + D) = AC + AD\)</span></li>
</ul>
</div>
<div id="intro-linalg-laws-matrix-transpose" class="section level4">
<h4><span class="header-section-number">2.3.2.5</span> Matrix Transpose</h4>
<p>Given a <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(A\)</span>, then the <span class="math inline">\(n\times m\)</span> matrix <span class="math inline">\(A^T\)</span> is called its <strong>transpose</strong>, if <span class="math inline">\((A^T)_{ij} = A_{ji}\)</span>. The matrix <span class="math inline">\(A\)</span> is called <strong>symmetric</strong>, if <span class="math inline">\(A = A^T\)</span>. For every matrix <span class="math inline">\(A\)</span> the transpose of the transpose is the matrix itself, i.e., <span class="math inline">\((A^T)^T = A\)</span>. For any <span class="math inline">\(m\times n\)</span> matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the transpose <span class="math inline">\((A+B)^T\)</span> of their sum <span class="math inline">\((A+B)\)</span> is computed as</p>
<p><span class="math display">\[(A+B)^T = A^T + B^T\]</span></p>
<p>For every <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(A\)</span> and every <span class="math inline">\(n\times p\)</span> matrix <span class="math inline">\(B\)</span>, it holds that</p>
<p><span class="math display">\[(AB)^T = B^T A^T\]</span></p>
</div>
<div id="intro-linalg-inverse-matrix" class="section level4">
<h4><span class="header-section-number">2.3.2.6</span> Inverse of a Matrix</h4>
<p>In this section, we are looking at square matrices. The <strong>inverse</strong> <span class="math inline">\(X\)</span> of a square matrix <span class="math inline">\(A\)</span> is defined as the square matrix that satisfies the condition <span class="math inline">\(AX = I\)</span>. If the inverse matrix <span class="math inline">\(X\)</span> exists, then the matrix <span class="math inline">\(A\)</span> is called invertable. If <span class="math inline">\(X\)</span> does not exist, <span class="math inline">\(A\)</span> is called singular. If the inverse of a matrix <span class="math inline">\(A\)</span> exists, it is uniquely determined and we call it <span class="math inline">\(A^{-1}\)</span>.</p>
<p>Let us assume two invertable <span class="math inline">\(n\times n\)</span> matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, then the following equations hold</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(A^{-1}A = I\)</span></li>
<li><span class="math inline">\(A^{-1}\)</span> is invertable and <span class="math inline">\((A^{-1})^{-1} = A\)</span></li>
<li><span class="math inline">\(I\)</span> is invertable and <span class="math inline">\(I^{-1} = I\)</span></li>
<li><span class="math inline">\(AB\)</span> is invertable and <span class="math inline">\((AB)^{-1} = B^{-1}A^{-1}\)</span></li>
<li><span class="math inline">\(A^T\)</span> is invertable and <span class="math inline">\((A^T)^{-1} = (A^{-1})^T\)</span></li>
</ol>
<p>For every square matrix <span class="math inline">\(A\)</span>, the following statements are equivalent.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(A\)</span> is invertable</li>
<li>The system of equations <span class="math inline">\(Ax = b\)</span> is solvable for every <span class="math inline">\(b\)</span>.</li>
<li>The system of equations <span class="math inline">\(Ax = 0\)</span> has only the trivial solution <span class="math inline">\(x=0\)</span>.</li>
</ol>
</div>
<div id="intro-linalg-orthogonal-matrix" class="section level4">
<h4><span class="header-section-number">2.3.2.7</span> Orthogonal Matrices</h4>
<p>A square matrix <span class="math inline">\(A\)</span> is called <strong>orthogonal</strong>, if the condition <span class="math inline">\(A^TA = I\)</span> holds. For two orthogonal matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the following statements hold.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(A\)</span> is invertable and <span class="math inline">\(A^{-1} = A^T\)</span></li>
<li><span class="math inline">\(A^{-1}\)</span> is orthogonal</li>
<li><span class="math inline">\(AB\)</span> is orthogonal</li>
<li><span class="math inline">\(I\)</span> is orthogonal</li>
</ol>
</div>
</div>
</div>
<div id="intro-linalg-systems-of-equations" class="section level2">
<h2><span class="header-section-number">2.4</span> Systems Of Equations</h2>
<p>Systems of linear equations are introduced based on <span class="citation">(Nipp and Stoffer <a href="#ref-Nipp2002" role="doc-biblioref">2002</a>)</span> and <span class="citation">(Searle <a href="#ref-Searle1971" role="doc-biblioref">1971</a>)</span>. Solving systems of linear equations is one of the fundamental tasks of linear algebra. We start with a general example of a system of linear equations which is given as</p>
<p><span class="math display" id="eq:intro-linalg-first-example">\[\begin{align}
 x_1 + 2x_2 &amp;= 5 \notag \\
2x_1 + 3x_2 &amp;= 8
\tag{2.1}
\end{align}\]</span></p>
<p>In <a href="intro-linalg.html#eq:intro-linalg-first-example">(2.1)</a> we are given a system of linear equations with two equations and two unknowns <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. The aim is to find numeric values for <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> such that both equations are satisfied. Inserting the values <span class="math inline">\(x_1 = 1\)</span> and <span class="math inline">\(x_2 = 2\)</span> into the above equations show that they are both satisfied. Hence the set <span class="math inline">\(L = \{x_1 = 1, x_2 = 2\}\)</span> consisting of the values for <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> that satisfy both equations is called a solution or a solution set for the above shown equations.</p>
<p>In general, a linear system of equations consists of <span class="math inline">\(m\)</span> equations and <span class="math inline">\(n\)</span> unknowns. In the example <a href="intro-linalg.html#eq:intro-linalg-first-example">(2.1)</a>, <span class="math inline">\(m=2\)</span> and <span class="math inline">\(n=2\)</span>.</p>
<p>The example in <a href="intro-linalg.html#eq:intro-linalg-no-solution">(2.2)</a> does not have any solutions.</p>
<p><span class="math display" id="eq:intro-linalg-no-solution">\[\begin{align}
 x_1 +  x_2 &amp;= 4 \notag \\
2x_1 + 2x_2 &amp;= 5
\tag{2.2}
\end{align}\]</span></p>
<p>This can be seen, that if the first equation in <a href="intro-linalg.html#eq:intro-linalg-no-solution">(2.2)</a> is multiplied by <span class="math inline">\(2\)</span>, we get <span class="math inline">\(2x_1 + 2x_2 = 8\)</span> which contradicts the second equation shown in <a href="intro-linalg.html#eq:intro-linalg-no-solution">(2.2)</a>.</p>
<p>A system with <span class="math inline">\(m=2\)</span> equations and <span class="math inline">\(n=3\)</span> unknowns in shown in <a href="intro-linalg.html#eq:intro-linalg-infnr-solution">(2.3)</a>.</p>
<p><span class="math display" id="eq:intro-linalg-infnr-solution">\[\begin{align}
 x_1 -  x_2 +  x_3 &amp;= 2 \notag \\
2x_1 +  x_2 -  x_3 &amp;= 4  
\tag{2.3}
\end{align}\]</span></p>
<p>There are infinitely many solutions consisting of <span class="math inline">\(x_1 = 2\)</span>, <span class="math inline">\(x_2 = \alpha\)</span> and <span class="math inline">\(x_3 = \alpha\)</span> for any real number <span class="math inline">\(\alpha\)</span>.</p>
<p>The examples in <a href="intro-linalg.html#eq:intro-linalg-first-example">(2.1)</a>, <a href="intro-linalg.html#eq:intro-linalg-no-solution">(2.2)</a> and <a href="intro-linalg.html#eq:intro-linalg-infnr-solution">(2.3)</a> already show all possible cases that may occur when solving linear systems of equations. The question is how to determine the set of all solutions of a system of linear equations.</p>
<div id="intro-linalg-matrix-vector-notation" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Matrix-Vector Notation</h3>
<p>So far, we have written systems of linear equations explicitly in the sense that every equation was written on one line. For small systems this is not a problem. But when the number of equations (<span class="math inline">\(m\)</span>) and the number of unknowns (<span class="math inline">\(n\)</span>) get very large, the explicit notation is no longer feasible. Hence, we need a notation that can also be used for large systems of equations. The so-called matrix-vector notation provides an efficient way to write down large systems of equations very efficiently.</p>
<p>We return to the example given by <a href="intro-linalg.html#eq:intro-linalg-first-example">(2.1)</a> and we define the matrix <span class="math inline">\(A\)</span> to be</p>
<p><span class="math display">\[
A = \left[
\begin{array}{cc}
1  &amp;  2 \\
2  &amp;  3
\end{array}
\right], 
\]</span></p>
<p>the vector <span class="math inline">\(x\)</span> to be</p>
<p><span class="math display">\[
x = \left[
\begin{array}{c}
x_1  \\
x_2  
\end{array}
\right], 
\]</span></p>
<p>and the vector <span class="math inline">\(y\)</span> to be</p>
<p><span class="math display">\[
y = \left[
\begin{array}{c}
5  \\
8  
\end{array}
\right], 
\]</span></p>
<p>With these definitions, we can write the system of equations given in <a href="intro-linalg.html#eq:intro-linalg-first-example">(2.1)</a> using matrix-vector notation as</p>
<p><span class="math display" id="eq:intro-linalg-matrix-vector-notation">\[\begin{equation}
A \cdot x = y
\tag{2.4}
\end{equation}\]</span></p>
</div>
</div>
<div id="intro-linalg-solving-systems-of-linear-equations" class="section level2">
<h2><span class="header-section-number">2.5</span> Solving Systems of Linear Equations</h2>
<p>If matrix <span class="math inline">\(A\)</span> in <a href="intro-linalg.html#eq:intro-linalg-matrix-vector-notation">(2.4)</a> is not singular, i.e. the inverse Matrix <span class="math inline">\(A^{-1}\)</span> of <span class="math inline">\(A\)</span> does exist, the solution <span class="math inline">\(x\)</span> to <a href="intro-linalg.html#eq:intro-linalg-matrix-vector-notation">(2.4)</a> can be written as <span class="math inline">\(x = A^{-1}y\)</span>. This result is obtained by pre-multiplying both sides of <a href="intro-linalg.html#eq:intro-linalg-matrix-vector-notation">(2.4)</a> with <span class="math inline">\(A^{-1}\)</span> and since a matrix times its inverse results in the identity matrix <span class="math inline">\(I\)</span>, the solution is obtained as</p>
<p><span class="math display" id="eq:intro-linalg-matrix-vector-notation-solution-derivation">\[\begin{align}
            A \cdot x  &amp;=  y \notag \\
A^{-1}\cdot A \cdot x  &amp;=  A^{-1} \cdot y \notag \\
            I \cdot x  &amp;=  A^{-1} \cdot y \notag \\
                    x  &amp;=  A^{-1} \cdot y
\tag{2.5}
\end{align}\]</span></p>
<p>For systems of equations with a singular matrix <span class="math inline">\(A\)</span>, solutions can be found, if the equations are <strong>consistent</strong>. The linear equations <span class="math inline">\(Ax = y\)</span> are consistent, if any linear relationship existing among the rows of <span class="math inline">\(A\)</span> also exist among the corresponding elements of <span class="math inline">\(y\)</span>. As a simple example, the equations</p>
<p><span class="math display">\[
\left[
\begin{array}{cc}
1  &amp;  2  \\
3  &amp;  6
\end{array}\right]
\left[
\begin{array}{c}
x_1  \\
x_2
\end{array}\right]
=
\left[
\begin{array}{c}
7  \\
21
\end{array}\right]
\]</span>
are consistent. In the matrix on the left the second row corresponds to three times the first row and in the vector on the right, the second element is also three times the first element. In contrast the equations</p>
<p><span class="math display">\[
\left[
\begin{array}{cc}
1  &amp;  2  \\
3  &amp;  6
\end{array}\right]
\left[
\begin{array}{c}
x_1  \\
x_2
\end{array}\right]
=
\left[
\begin{array}{c}
7  \\
24
\end{array}\right]
\]</span>
are not consistent. From this example, we can already see that non-consistent equations do not have any solutions. But consistent equations <span class="math inline">\(Ax = y\)</span> have a solution which can be written as <span class="math inline">\(x = Gy\)</span> if and only if, <span class="math inline">\(AGA = A\)</span> which means that <span class="math inline">\(G\)</span> is a so-called generalized inverse of <span class="math inline">\(A\)</span>. The matrix <span class="math inline">\(G\)</span> is often written as <span class="math inline">\(A^-\)</span>. The proof of this statement is given on page 9 of <span class="citation">(Searle <a href="#ref-Searle1971" role="doc-biblioref">1971</a>)</span>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-3blue1brown2016">
<p>3blue1brown. 2016. “What are vectors.” <a href="https://youtu.be/fNk%7B\_%7DzzaMoSs">https://youtu.be/fNk{\_}zzaMoSs</a>.</p>
</div>
<div id="ref-Nipp2002">
<p>Nipp, Kaspar, and Daniel Stoffer. 2002. <em>Lineare Algebra</em>. 5th ed. Zurich: vdf Hochschulverlag.</p>
</div>
<div id="ref-Searle1971">
<p>Searle, S R. 1971. <em>Linear Models</em>. Wiley Clas. New York: John Wiley &amp; Sons.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="asm-intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="quan-gen.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/charlotte-ngs/gelasmss2021/edit/master/cn_asm/92_intro_linalg.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-asmas-ss2021.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
